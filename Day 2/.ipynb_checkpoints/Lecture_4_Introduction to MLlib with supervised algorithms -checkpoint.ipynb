{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16c490e4"
   },
   "source": [
    "## Why Spark for Machine Learning?\n",
    "- Spark is a unified analytics engine that provides an ecosystem for data ingestion, feature engineering, model training, and deployment. Without Spark, developers would\n",
    "need many disparate tools to accomplish this set of tasks, and might still struggle with\n",
    "scalability.\n",
    "<br>\n",
    "- Spark has two machine learning packages: <b>spark.mllib</b> and <b>spark.ml</b>. <b>spark.mllib</b>\n",
    "is the original machine learning API, <b>based on the RDD API</b> (which has been in\n",
    "maintenance mode since Spark 2.0), while <b>spark.ml</b> is the <b><i>newer</b></i> API, <b>based on DataFrames</b>. We will focus on using the <b>spark.ml</b> package and how to\n",
    "design machine learning pipelines in Spark. However, we use “MLlib” as an umbrella\n",
    "term to refer to both machine learning library packages in Apache Spark.\n",
    "- With spark.ml , data scientists can use one ecosystem for their data preparation and\n",
    "model building, without the need to downsample their data to fit on a single\n",
    "machine.\n",
    "- Spark.ml focuses on O(n) scale-out, where the model scales linearly with\n",
    "the number of data points you have, so it can scale to massive amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "580f92c3"
   },
   "source": [
    "## Designing Machine Learning Pipelines\n",
    "- The concept of\n",
    "pipelines is common across many ML frameworks as a way to organize a series of\n",
    "operations to apply to your data.\n",
    "- In <b>MLlib, the Pipeline API</b> provides a high-level API\n",
    "built on top of DataFrames to organize your machine learning workflow.\n",
    "- The Pipeline API is composed of a series of <b>transformers and estimators</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58473d80"
   },
   "source": [
    "<b>Throughout this session, we will use the San Francisco housing data set from Inside\n",
    "Airbnb. It contains information about Airbnb rentals in San Francisco, such as the\n",
    "number of bedrooms, location, review scores, etc., and our goal is to build a model to\n",
    "predict the nightly rental prices for listings in that city. This is a regression problem,\n",
    "because price is a continuous variable. We will guide you through the workflow a data\n",
    "scientist would go through to approach this problem, including feature engineering,building models, hyperparameter tuning, and evaluating model performance.</b><br><br>\n",
    "<b><font color='red'>Note: </font></b>This\n",
    "data set is quite messy and can be difficult to model (like most real-world data sets!),\n",
    "so if you are experimenting on your own, don’t feel bad if your early models aren’t\n",
    "great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dce8379"
   },
   "source": [
    "The intent of this session is not to show you every API in MLlib, but rather to equip\n",
    "you with the skills and knowledge to get started with using MLlib to build end-to-end\n",
    "pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e73e9908"
   },
   "source": [
    "## MLlib terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ba8a57f"
   },
   "source": [
    "### Transformer\n",
    "- Accepts a DataFrame as input, and returns a new DataFrame with one or more\n",
    "columns appended to it. \n",
    "- Transformers do not learn any parameters from your data and simply apply rule-based transformations to either\n",
    "    - <b>prepare data for model training</b> or\n",
    "    - <b>generate predictions using a trained MLlib model.</b> \n",
    "- They have <b>a .transform()</b> method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42ab1699"
   },
   "source": [
    "### Estimator\n",
    "- <b>Learns (or “fits”)</b> parameters from your DataFrame via <b>a .fit()</b> method and\n",
    "returns a Model , which is a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b43e86f6"
   },
   "source": [
    "### Pipeline\n",
    "- Organizes a series of transformers and estimators into a single model. While\n",
    "pipelines themselves are estimators, the output of pipeline.fit() returns a Pipeline Model , a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3886d452"
   },
   "source": [
    "## Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8b75b83"
   },
   "source": [
    "### The used data is slightly preprocessed as follow:   <i><font color='green'>(you can try by yourself)</font><i/>\n",
    "- Outliers are removed (e.g., Airbnbs posted for $0/night).\n",
    "- All integers are converted to doubles.\n",
    "- An informative subset of the more than one hundred fields.\n",
    "- For any missing numerical values in the data columns, the median value is imputed and added\n",
    "an indicator column (the column name followed by _na , such as bedrooms_na ). This way the ML model or human analyst can interpret any value in that column as an imputed value, not a true value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b77c3e9f"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4c64b4f7"
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f0270227"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3a3ee17c"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Mllib_Airbnb').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69be4a68"
   },
   "outputs": [],
   "source": [
    "filepath = '/home/hhhhh/Data/sf-airbnb/sf-airbnb-clean.parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1dce993"
   },
   "outputs": [],
   "source": [
    "airbnbDF = spark.read.parquet(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbf6a37",
    "outputId": "ed17b5a2-bfe7-4e06-bc55-7fb331e55cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- number_of_reviews: double (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms_na: double (nullable = true)\n",
      " |-- bathrooms_na: double (nullable = true)\n",
      " |-- beds_na: double (nullable = true)\n",
      " |-- review_scores_rating_na: double (nullable = true)\n",
      " |-- review_scores_accuracy_na: double (nullable = true)\n",
      " |-- review_scores_cleanliness_na: double (nullable = true)\n",
      " |-- review_scores_checkin_na: double (nullable = true)\n",
      " |-- review_scores_communication_na: double (nullable = true)\n",
      " |-- review_scores_location_na: double (nullable = true)\n",
      " |-- review_scores_value_na: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a546b1b6",
    "outputId": "db9cb7ca-bc0e-4e60-d6bb-3064fc653be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "\"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "122a7ff7",
    "outputId": "f49fb21e-da06-4273-8470-56d423b59e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF.createOrReplaceTempView('airbnbDF_view')\n",
    "spark.sql('SELECT neighbourhood_cleansed, room_type, bedrooms, bathrooms,number_of_reviews, price FROM airbnbDF_view').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69115a3b"
   },
   "source": [
    "### Our goal is to predict the price per night for a rental property, given our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4f15b5d"
   },
   "source": [
    "### Creating Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6b27312",
    "outputId": "19f4ba82-7631-4c45-8b2d-c0a434c5f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8,.2],seed=42)\n",
    "print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2f05e8b"
   },
   "source": [
    "- But what happens if we change the number of executors in our Spark cluster? The\n",
    "Catalyst optimizer determines the optimal way to partition your data as a function of\n",
    "your cluster resources and size of your data set. Given that data in a Spark DataFrame\n",
    "is row-partitioned and each worker performs its split independently of the other\n",
    "workers, if the data in the partitions changes, then the result of the split (by random\n",
    "Split() ) won’t be the same.\n",
    "- While you could fix your cluster configuration and your seed to ensure that you get\n",
    "consistent results, it is recommended to split your data once, then write it out to\n",
    "its own train/test folder so you don’t have these reproducibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7acadd9"
   },
   "source": [
    "<b><font color = 'red'>Note: </font>\n",
    "During your exploratory analysis, you should cache the training\n",
    "data set because you will be accessing it many times throughout the\n",
    "machine learning process.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd7065dc"
   },
   "source": [
    "### Preparing Features with Transformers\n",
    "- Now that we have split our data into training and test sets, let’s prepare the data to\n",
    "build a linear regression model predicting price given the number of bedrooms.\n",
    "-  Linear regression (like many other algorithms in\n",
    "Spark) requires that all the input features are contained within a single vector in your\n",
    "DataFrame. Thus, we need to transform our data.\n",
    "- Transformers in Spark accept a DataFrame as input and return a new DataFrame\n",
    "with one or more columns appended to it. They do not learn from your data, but\n",
    "apply rule-based transformations using the transform() method.\n",
    "- For the task of putting all of our features into a single vector, we will use the <b>VectorAssembler</b> transformer. \n",
    "- VectorAssembler takes a list of input columns and creates a new DataFrame with an additional column, which we will call features . It combines the values of those input columns into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8609a94f"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fba14acf"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=['bedrooms'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "320ad2c2",
    "outputId": "7b9d0346-6e08-4675-9729-573dcf1d0bc1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_cfc9ceb9b339"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac5cda57"
   },
   "outputs": [],
   "source": [
    "vectTrainDF = vecAssembler.transform(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9806232c",
    "outputId": "9b4ab94c-0fd9-43ac-83cd-fe6c184d8d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- number_of_reviews: double (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms_na: double (nullable = true)\n",
      " |-- bathrooms_na: double (nullable = true)\n",
      " |-- beds_na: double (nullable = true)\n",
      " |-- review_scores_rating_na: double (nullable = true)\n",
      " |-- review_scores_accuracy_na: double (nullable = true)\n",
      " |-- review_scores_cleanliness_na: double (nullable = true)\n",
      " |-- review_scores_checkin_na: double (nullable = true)\n",
      " |-- review_scores_communication_na: double (nullable = true)\n",
      " |-- review_scores_location_na: double (nullable = true)\n",
      " |-- review_scores_value_na: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectTrainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a930746c",
    "outputId": "e7820053-3be1-4176-d480-f76091b42b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectTrainDF.select('bedrooms','features','price').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c946de2"
   },
   "source": [
    "### Using Estimators to Build Models\n",
    "- After setting up our vectorAssembler , we have our data prepared and transformed into a format that our linear regression model expects.\n",
    "- In Spark, <b>LinearRegression</b> is a type of <b>estimator</b>—it takes in a <b>DataFrame</b> and returns a <b>Model</b>.\n",
    "- Estimators learn parameters from your data, have an <b>estimator_name.fit()</b> method.\n",
    "\n",
    "<b><font color = 'red'>Note: </font>Estimators are eagerly evaluated (i.e., kick off Spark jobs), whereas transformers are lazily evaluated.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "baa60ba6"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "889dc156"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='features',labelCol='price')\n",
    "lrModel = lr.fit(vectTrainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a6ca91e"
   },
   "source": [
    "- <b>lr.fit()</b> returns a <b>LinearRegressionModel ( lrModel )</b>, which is a <b>transformer</b>. In other words, the output of an estimator’s <b>fit()</b> method is a <b>transformer</b>.\n",
    "- Once the estimator has learned the parameters, the transformer can apply these parameters to new data points to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb5d6e68",
    "outputId": "aac339fe-0dc7-4b19-97a8-e152916226f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is \u001b[1m price = 47.51 + 123.68*bedrooms \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "m =round(lrModel.coefficients[0],2)\n",
    "b = round(lrModel.intercept,2)\n",
    "print(f\"The formula for the linear regression line is \\033[1m price = {b} + {m}*bedrooms \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a31df680"
   },
   "source": [
    "## Creating a Pipeline\n",
    "- If we want to apply our model to our test set, then we need to prepare that data in the same way as the training set (i.e., pass it through the vector assembler).\n",
    "- Oftentimes data preparation pipelines will have multiple steps, and it becomes cumbersome to remember not only which steps to apply, but also the ordering of the steps.\n",
    "- This is the motivation for the Pipeline API: you simply specify the stages you want your data to pass through, in order, and Spark takes care of the processing for you.\n",
    "- They provide the user with better code reusability and organization.\n",
    "- In Spark, <b>Pipelines are estimators, whereas PipelineModels—fitted Pipelines—are transformers</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb9fc55f"
   },
   "outputs": [],
   "source": [
    "# Building the pipeline\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58567f5b"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[vecAssembler,lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8933c78"
   },
   "source": [
    "- Another advantage of using the Pipeline API is that it determines which stages are <b>estimators/transformers</b> for you, so you don’t have to worry about specifying <b>name.fit()</b> versus <b>name.transform()</b> for each of the stages.\n",
    "- Since pipelineModel is a transformer, it is straightforward to apply it to our test data set too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d3deb34"
   },
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d3c90ca",
    "outputId": "40dac2c0-8e97-4a6a-96ea-b9ca9f6cbf7e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF.select(\"bedrooms\",'features','price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "521b4b81"
   },
   "source": [
    "## Multivariable Linear Regression\n",
    "In the pipeline we just created, we only had two stages, and our linear regression\n",
    "model only used one feature. Let’s take a look at how to build a slightly more complex\n",
    "pipeline that incorporates all of our numeric and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9483711"
   },
   "source": [
    "### One-hot encoding\n",
    "- Most machine learning models in MLlib expect numerical values as input, represented as vectors.\n",
    "- To convert categorical values into numeric values, we can use a\n",
    "technique called <b>one-hot encoding (OHE)</b>.\n",
    "- Suppose we have a column called Animal\n",
    "and we have three types of animals: Dog , Cat , and Fish . We can’t pass the string types\n",
    "into our ML model directly, so we need to assign a numeric mapping, such as this:<br>\n",
    "<center><b>Animal = {\"Dog\", \"Cat\", \"Fish\"}</b></center>\n",
    "<center><b>\"Dog\" = 1, \"Cat\" = 2, \"Fish\" = 3</b></center>\n",
    "- However, using this approach we’ve introduced some <b>spurious relationships</b> into our\n",
    "data set that weren’t there before. For example, <b>why did we assign Cat twice the value\n",
    "of Dog ?</b> The numeric values we use should not introduce any relationships into our\n",
    "data set. Instead, we want to create a separate column for every distinct value in our\n",
    "Animal column:\n",
    "<center><b>\"Dog\" = [ 1, 0, 0]</b></center>\n",
    "<center><b>\"Cat\" = [ 0, 1, 0]</b></center>\n",
    "<center><b>\"Fish\" = [0, 0, 1]</b></center>\n",
    "If the animal is a dog, it has a one in the first column and zeros elsewhere. If it is a cat,\n",
    "it has a one in the second column and zeros elsewhere. The ordering of the columns\n",
    "is irrelevant. It is as <b>pandas.get_dummies()</b>.<br>\n",
    "If we had a zoo of <b>300 animals</b>, would <b>OHE</b> massively increase consumption of memory/compute resources? <b><i>Not with Spark!</i></b> Spark internally uses a <b>SparseVector</b> when\n",
    "the majority of the entries are 0 , <i>as is often the case after OHE</i>, so it does not waste\n",
    "space storing 0 values. Let’s take a look at an example to better understand how\n",
    "SparseVector s work:\n",
    "<center><b>DenseVector(0, 0, 0, 7, 0, 2, 0, 0, 0, 0)</b></center>\n",
    "<center><b>SparseVector(10, [3, 5], [7, 2])</b></center>\n",
    "The <b>DenseVector</b> in this example contains 10 values, all but 2 of which are 0 . To create a SparseVector , we need to keep track of the size of the vector, the indices of the\n",
    "nonzero elements, and the corresponding values at those indices. <i>In this example the\n",
    "size of the vector is 10, there are two nonzero values at indices 3 and 5, and the corresponding values at those indices are 7 and 2.</i><br>\n",
    "<br>A common approach to one-hot encode your data with Spark is to use the <b>StringIndexer and OneHotEncoder</b>. With this approach, the first step is to\n",
    "apply the <b>StringIndexer <i>estimator<i></b> to convert categorical values into category indices. These category indices are ordered by label frequencies, so the most frequent\n",
    "label gets index 0, which provides us with reproducible results across various runs of\n",
    "the same data.<br>\n",
    "Once you have created your category indices, you can pass those as input to the\n",
    "<b>OneHotEncoder</b>. The <b>OneHotEncoder<i> (estimator)</i></b> maps a column of category indices to a column of binary vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "724ce686"
   },
   "source": [
    "<b><font color='red'>Note: </font> In our data set, any column of type string is treated as a categorical feature, but sometimes you might have numeric features you want treated as categorical or vice versa.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "error",
     "timestamp": 1633024569606,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "39b2602f",
    "outputId": "c61b1ddb-6d34-4b45-d23a-dfc6ab4b9b00"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 805,
     "status": "aborted",
     "timestamp": 1633024569544,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "8a192bb3"
   },
   "outputs": [],
   "source": [
    "trainDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 812,
     "status": "aborted",
     "timestamp": 1633024569552,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "4d0ce9ff"
   },
   "outputs": [],
   "source": [
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes\n",
    "                   if dataType == \"string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "aborted",
     "timestamp": 1633024569554,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "e142879a"
   },
   "outputs": [],
   "source": [
    "categoricalCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "aborted",
     "timestamp": 1633024569555,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "c1e846e7"
   },
   "outputs": [],
   "source": [
    "indexOutputCols = [x + \"_Index\" for x in categoricalCols]\n",
    "indexOutputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 817,
     "status": "aborted",
     "timestamp": 1633024569559,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "082082c1"
   },
   "outputs": [],
   "source": [
    "oheOutputCols = [x + \"_OHE\" for x in categoricalCols]\n",
    "oheOutputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 818,
     "status": "aborted",
     "timestamp": 1633024569561,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "59cf2a6d"
   },
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                             outputCols=indexOutputCols,\n",
    "                             handleInvalid='skip')\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                          outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 821,
     "status": "aborted",
     "timestamp": 1633024569564,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "eb6af67f"
   },
   "outputs": [],
   "source": [
    "numericCols = [field for (field,dataType) in trainDF.dtypes\n",
    "              if ((dataType=='double')& (field!='price'))]\n",
    "numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 821,
     "status": "aborted",
     "timestamp": 1633024569565,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "0ac83ab1"
   },
   "outputs": [],
   "source": [
    "assemblerInputs = oheOutputCols + numericCols\n",
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 820,
     "status": "aborted",
     "timestamp": 1633024569566,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "9f2a5b2c"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0535e465"
   },
   "source": [
    "- Now you might be wondering, “How does the StringIndexer handle new categories that appear in the test data set, but not in the training data set?” \n",
    "- There is a handleInvalid parameter that specifies how you want to handle them. \n",
    "- The option are <b><i>skip (filter out rows with invalid data), error (throw an error), or keep (put invalid data in a special additional bucket, at index numLabels )</b></i>. \n",
    "- For this example, we just skipped the invalid records.\n",
    "- <b>For more information and detailed explanation check the following resources:</b><br>\n",
    "https://stackoverflow.com/questions/34681534/spark-ml-stringindexer-handling-unseen-labels<br>\n",
    "https://spark.apache.org/docs/latest/ml-features.html#stringindexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 821,
     "status": "aborted",
     "timestamp": 1633024569568,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "c55dc61a"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='price',featuresCol='features')\n",
    "pipeline =Pipeline(stages = [stringIndexer,oheEncoder,vecAssembler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 821,
     "status": "aborted",
     "timestamp": 1633024569569,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "bd671070"
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "aborted",
     "timestamp": 1633024569572,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "4fcff006"
   },
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 826,
     "status": "aborted",
     "timestamp": 1633024569574,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "51840ee5"
   },
   "outputs": [],
   "source": [
    "predDF.select('features','price','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 855,
     "status": "aborted",
     "timestamp": 1633024569605,
     "user": {
      "displayName": "Hatem Elattar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04066847731966484714"
     },
     "user_tz": -120
    },
    "id": "03696ce2"
   },
   "outputs": [],
   "source": [
    "predDF.select('features').show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb0d65ea"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1372fbc4"
   },
   "outputs": [],
   "source": [
    "# Using RMSE\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adb58342"
   },
   "outputs": [],
   "source": [
    "regressionEvaluator = RegressionEvaluator(predictionCol='prediction',\n",
    "                                         labelCol='price',\n",
    "                                         metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1afde29b",
    "outputId": "d8a0c37e-29cb-4637-cbfb-c248c6daa9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "#print(\"RMSE is {:.1f}\".format(rmse))\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dc7c99e",
    "outputId": "02ba6cd9-c220-4f10-8998-33f13628bbf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "# Using R^2\n",
    "r2 = RegressionEvaluator(predictionCol='prediction',\n",
    "                                         labelCol='price',\n",
    "                                         metricName='r2').evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c4f47f8"
   },
   "source": [
    "- Our R2 is positive, but it’s very close to 0. One of the reasons why our model is not performing too well is because our label, price , appears to be <b>log-normally distributed</b>.\n",
    "- If a distribution is log-normal, it means that if we take the logarithm of the\n",
    "value, the result looks like a normal distribution.\n",
    "- Price is often log-normally distributed. If you think about rental prices in San Francisco, most cost around $200 per\n",
    "night, but there are some that rent for thousands of dollars a night!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4666a9cf"
   },
   "outputs": [],
   "source": [
    "price  = airbnbDF.select('price').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95e22461"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48e2f645",
    "outputId": "01c1d6ec-0b9f-47f1-f7f2-90ce532124ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='price', ylabel='Count'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(price['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "be0a58b9"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6920c306"
   },
   "outputs": [],
   "source": [
    "price['log_price']= price['price'].apply(math.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85b65af5",
    "outputId": "5e4edf1d-df35-4169-aae1-38d5cbb0c754"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>log_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170.0</td>\n",
       "      <td>5.135798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>235.0</td>\n",
       "      <td>5.459586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>785.0</td>\n",
       "      <td>6.665684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  log_price\n",
       "0  170.0   5.135798\n",
       "1  235.0   5.459586\n",
       "2   65.0   4.174387\n",
       "3   65.0   4.174387\n",
       "4  785.0   6.665684"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2328bb4a",
    "outputId": "c34779c2-330e-423f-a85d-3cb8756bce81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='log_price', ylabel='Count'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(price['log_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceff4ace"
   },
   "source": [
    "## Exercise:\n",
    "### Try building a model to predict price on the log scale, then exponentiate the prediction to get it out of log scale and evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "be839c7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fb5230e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afa0950c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d1bcac2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50475fb1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cabc216a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f8beb48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00dbaa61"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a178e124"
   },
   "source": [
    "### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc0ca663"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "pipelinePath = \"lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f426d184"
   },
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60354328"
   },
   "source": [
    "## Decision trees\n",
    "As an off-the-shelf solution, decision trees are well suited to data mining. They are\n",
    "relatively fast to build, highly interpretable, and scale-invariant (i.e., standardizing or\n",
    "scaling the numeric features does not change the performance of the tree). <b>So what is\n",
    "a decision tree?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d078efa2"
   },
   "source": [
    "- <b>A decision tree</b> is a series of if-then-else rules learned from your data for <b>classification\n",
    "or regression tasks.</b>\n",
    "- Suppose we are trying to build a model to predict whether or not\n",
    "someone will accept a job offer, and the features comprise salary, commute time, free\n",
    "coffee, etc. If we fit a decision tree to this data set, we might get a model that looks\n",
    "like\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- The depth of a decision tree is the longest path from the root node to any given leaf\n",
    "node. In this figure, the depth is three.\n",
    "- Trees that are very deep are prone to overfitting, or memorizing noise in your training data set, but trees that are too shallow will\n",
    "underfit to your data set (i.e., could have picked up more signal from the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "862d0d30"
   },
   "source": [
    "<b><i>\n",
    "- For decision trees, you don’t have to worry about standardizing or scaling your input features, because this has no impact on the splits—but you do have to be careful about how you prepare your categorical features.\n",
    "- Tree-based methods can naturally handle categorical variables. In spark.ml , you just\n",
    "need to pass the categorical columns to the StringIndexer , and the decision tree can\n",
    "take care of the rest.</b></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "653a5650"
   },
   "source": [
    "### Let’s fit a decision tree to our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2977689b"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41593866"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(labelCol='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67d85e3c"
   },
   "outputs": [],
   "source": [
    "# Filter for just numeric columns (and exclude price, our label)\n",
    "numericCols =[field for (field,dataType) in trainDF.dtypes \n",
    "              if ((dataType=='double')&(field!='price'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc612401"
   },
   "outputs": [],
   "source": [
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "assemblerInputs = indexOutputCols + numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "402fdf0d"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf6857fc"
   },
   "outputs": [],
   "source": [
    "# Combine stages into pipeline\n",
    "stages = [stringIndexer, vecAssembler, dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d50576c1",
    "outputId": "ee6d684e-57e5-46d0-eb92-d323e98de8ab",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-11982d4acd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line should error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(trainDF) # This line should error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae993f49"
   },
   "source": [
    "- We can see that there is an issue with the <b>maxBins</b> parameter. What does that parameter do? \n",
    "- <b>maxBins</b> determines the number of bins into which your continuous features are discretized, or split. This discretization step is crucial for performing distributed training. \n",
    "- There is no maxBins parameter in <b>scikit-learn</b> because all of the data and the model reside on a single machine. \n",
    "- In Spark, however, workers have all the columns of the data, but only a subset of the rows. Thus, when communicating about which features and values to split on, we need to be sure they’re all talking about the same split values, which we get from the common discretization set up at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "285619b1"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb0cde84"
   },
   "source": [
    "- Every worker has to compute summary statistics for every feature and every possible split point, and those statistics will be aggregated across the workers.\n",
    "- MLlib requires maxBins to be large enough to handle the discretization of the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ba80e6c",
    "outputId": "3647459e-dbc3-4999-e613-2776fb6c875c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- number_of_reviews: double (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms_na: double (nullable = true)\n",
      " |-- bathrooms_na: double (nullable = true)\n",
      " |-- beds_na: double (nullable = true)\n",
      " |-- review_scores_rating_na: double (nullable = true)\n",
      " |-- review_scores_accuracy_na: double (nullable = true)\n",
      " |-- review_scores_cleanliness_na: double (nullable = true)\n",
      " |-- review_scores_checkin_na: double (nullable = true)\n",
      " |-- review_scores_communication_na: double (nullable = true)\n",
      " |-- review_scores_location_na: double (nullable = true)\n",
      " |-- review_scores_value_na: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4e59810",
    "outputId": "39cd19de-0154-461a-f57c-578bb2de9bf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select('neighbourhood_cleansed').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcbef742"
   },
   "outputs": [],
   "source": [
    "# We do not have to rrdefine it completely we can just setMaxBins \n",
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e060ad7",
    "outputId": "e5a95840-a9c9-4baa-ad91-b3b01e43aa60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_9d5b6ac076a3, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have successfully built our model, \n",
    "we can extract the if-then-else rules learned by the decision tree:\n",
    "\"\"\" \n",
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "574249c3"
   },
   "source": [
    "- You’ll notice that it’s possible to split on the\n",
    "same feature more than once (e.g., feature 12), but at different split values. Also\n",
    "notice the difference between how the decision tree splits on numeric features versus\n",
    "categorical features: for numeric features it checks if the value is less than or equal to\n",
    "the threshold, and for categorical features it checks if the value is in that set or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a9beb61",
    "outputId": "ec573f09-be05-4541-b4bc-910c1aba73d4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policy_Index</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookable_Index</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_type_Index</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansed_Index</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_type_Index</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhost_Index</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_type_Index</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  importance\n",
       "12                        bedrooms    0.283406\n",
       "1        cancellation_policy_Index    0.167893\n",
       "2           instant_bookable_Index    0.140081\n",
       "4              property_type_Index    0.128179\n",
       "15               number_of_reviews    0.126233\n",
       "3     neighbourhood_cleansed_Index    0.056200\n",
       "9                        longitude    0.038810\n",
       "14                  minimum_nights    0.029473\n",
       "13                            beds    0.015218\n",
       "5                  room_type_Index    0.010905\n",
       "10                    accommodates    0.003603\n",
       "0          host_is_superhost_Index    0.000000\n",
       "24                    bathrooms_na    0.000000\n",
       "25                         beds_na    0.000000\n",
       "26         review_scores_rating_na    0.000000\n",
       "27       review_scores_accuracy_na    0.000000\n",
       "28    review_scores_cleanliness_na    0.000000\n",
       "22             review_scores_value    0.000000\n",
       "29        review_scores_checkin_na    0.000000\n",
       "30  review_scores_communication_na    0.000000\n",
       "31       review_scores_location_na    0.000000\n",
       "23                     bedrooms_na    0.000000\n",
       "16            review_scores_rating    0.000000\n",
       "21          review_scores_location    0.000000\n",
       "20     review_scores_communication    0.000000\n",
       "19           review_scores_checkin    0.000000\n",
       "18       review_scores_cleanliness    0.000000\n",
       "17          review_scores_accuracy    0.000000\n",
       "11                       bathrooms    0.000000\n",
       "8                         latitude    0.000000\n",
       "7        host_total_listings_count    0.000000\n",
       "6                   bed_type_Index    0.000000\n",
       "32          review_scores_value_na    0.000000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can also extract the feature importance scores from our model to see the most\n",
    "important features:\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "featureImp = pd.DataFrame(list(zip(vecAssembler.getInputCols(),dtModel.featureImportances)),\n",
    "                         columns=['feature','importance'])\n",
    "featureImp.sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "387a7b44"
   },
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d997b90",
    "outputId": "e8fc1a55-84ad-440e-bd7f-b9f180a0ee9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(33,[1,3,4,7,8,9,...| 85.0|131.96658097686375|\n",
      "|[0.0,2.0,0.0,15.0...| 45.0|104.23992784125075|\n",
      "|[0.0,2.0,0.0,15.0...| 70.0|104.23992784125075|\n",
      "|(33,[1,3,5,7,8,9,...|128.0|104.23992784125075|\n",
      "|(33,[1,3,4,5,7,8,...|159.0|104.23992784125075|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF.select('features','price','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7eb3a18",
    "outputId": "3c5f5b4e-bdd1-4699-ba20-113871b5260e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is -1.5696388432265533\n"
     ]
    }
   ],
   "source": [
    "# Using R^2\n",
    "r2 = RegressionEvaluator(predictionCol='prediction',\n",
    "                                         labelCol='price',\n",
    "                                         metricName='r2').evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23928410"
   },
   "source": [
    "## Random forests\n",
    "- Ensembles work by taking a democratic approach. \n",
    "- Imagine there are many M&Ms in\n",
    "a jar. You ask one hundred people to guess the number of M&Ms, and then take the\n",
    "average of all the guesses. The average is probably closer to the true value than most\n",
    "of the individual guesses. \n",
    "- That same concept applies to machine learning models. If\n",
    "you build many models and combine/average their predictions, they will be more\n",
    "robust than those produced by any individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b8a2a86"
   },
   "source": [
    "<b>Random forests</b> are an ensemble of decision trees with two key tweaks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93810105"
   },
   "source": [
    "### Bootstrapping samples by rows\n",
    "- <i><b>Bootstrapping</b></i> is a technique for simulating new data by sampling with replacement from your original data. Each decision tree is trained on a different bootstrap sample of your data set, which produces slightly different decision trees,\n",
    "and then you aggregate their predictions. This technique is known as bootstrap\n",
    "aggregating, or bagging. In a typical random forest implementation, each tree\n",
    "samples the same number of data points with replacement from the original data\n",
    "set, and that number can be controlled through the subsamplingRate parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f30defa2"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "253d8f49"
   },
   "source": [
    "### Random feature selection by columns\n",
    "- The main drawback with bagging is that the trees are all highly correlated, and\n",
    "thus learn similar patterns in your data. To mitigate this problem, each time you\n",
    "want to make a split you only consider a random subset of the columns (1/3 of\n",
    "the features for RandomForestRegressor and $\\sqrt{no. features}$ for RandomForestClassifier). Due to this randomness you introduce, you typically want each tree to\n",
    "be quite shallow. You might be thinking: each of these trees will perform worse\n",
    "than any single decision tree, so how could this approach possibly be better? It\n",
    "turns out that each of the trees learns something different about your data set,\n",
    "and combining this collection of “weak” learners into an ensemble makes the forest much more robust than a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e74a3773"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "327b026a"
   },
   "source": [
    "### The APIs for random forests and decision trees are similar, and both can be applied to regression or classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "504f0fa1"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a354244d"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(labelCol='price',maxBins=40,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f8b8ab3"
   },
   "source": [
    "- Once you’ve trained your random forest, you can pass new data points through the\n",
    "different trees trained in the ensemble.\n",
    "- If you build a random forest it passes the\n",
    "test point through each of the trees in the forest and  simply averages those predictions.\n",
    "- Even though each of these trees is less performant\n",
    "than any individual decision tree, the collection (or ensemble) actually provides a\n",
    "more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40739548"
   },
   "source": [
    "<b><i><font color='blue'>Random forests truly demonstrate the power of distributed machine learning with\n",
    "Spark, as each tree can be built independently of the other trees (e.g., you do not need\n",
    "to build tree 3 before you build tree 10). Furthermore, within each level of the tree,\n",
    "you can parallelize the work to find the optimal splits.</font></i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "044c5f6d"
   },
   "source": [
    "## Exercise\n",
    "- Build the pipeline using <b><i>RandomForestRegressor</i></b> and evaluate the model performance.\n",
    "- Perform data preparation steps.\n",
    "    1. StringIndexer.\n",
    "    2. Columns selection.\n",
    "    3. VectorAssembler.\n",
    "    4. Model Creation.\n",
    "    5. Combine stages into pipeline.\n",
    "    6. Model Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85ddd6f8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "848b911b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05b3de7d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c8f87d5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20396573"
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3c6bd3a"
   },
   "source": [
    "### So how do we determine what the optimal number of trees in our random forest or the max depth of those trees should be? This process is called hyperparameter tuning.\n",
    "- In contrast to a parameter, a hyperparameter is a value that controls the learning pro‐\n",
    "cess or structure of your model, and it is not learned during training.\n",
    "- Both the num‐\n",
    "ber of trees and the max depth are examples of hyperparameters you can tune for\n",
    "random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1ef3b85"
   },
   "source": [
    "## k-Fold Cross-Validation\n",
    "- Which data set should we use to determine the optimal hyperparameter values? If we\n",
    "use the training set, then the model is likely to overfit, or memorize the nuances of\n",
    "our training data. This means it will be less likely to generalize to unseen data. But if\n",
    "we use the test set, then that will no longer represent “unseen” data, so we won’t be\n",
    "able to use it to verify how well our model generalizes. Thus, we need another data set\n",
    "to help us determine the optimal hyperparameters: <b>the validation data set.</b>\n",
    "- For example, instead of splitting our data into an <b>80/20 train/test split</b>, as we did earlier, we can do a <b>60/20/20 split</b> to generate <b>training, validation, and test data sets</b>,\n",
    "respectively.\n",
    "- We can then build our model on the training set, evaluate performance\n",
    "on the validation set to select the best hyperparameter configuration, and apply the\n",
    "model to the test set to see how well it performs on new data.\n",
    "- However, one of the\n",
    "downsides of this approach is that we lose <b>25%</b> of our training data <b>(80% -> 60%)</b>,\n",
    "which could have been used to help improve the model. This motivates the use of the\n",
    "<b>k-fold cross-validation</b> technique to solve this problem.\n",
    "- With this approach, instead of splitting the data set into separate training, validation,\n",
    "and test sets, we split it into training and test sets as before—but we use the training\n",
    "data for both training and validation. To accomplish this, we split our training data\n",
    "into <b>k subsets, or “folds” (e.g., three)</b>. Then, for a given hyperparameter configuration,\n",
    "we train our model on <b>k–1</b> folds and evaluate on the remaining fold, repeating this\n",
    "process <b>k times</b>.\n",
    "![image.png](attachment:image.png)\n",
    "- As this figure shows, if we split our data into three folds, our model is first trained on\n",
    "the first and second folds (or splits) of the data, and evaluated on the third fold. We\n",
    "then build the same model with the same hyperparameters on the first and third folds of the data, and evaluate its performance on the second fold.\n",
    "- Lastly, we build the model on the second and third folds and evaluate it on the first fold. We then average\n",
    "the performance of those <b>three (or k)</b> validation data sets as a proxy of how well this\n",
    "model will perform on unseen data, as every data point had the chance to be part of\n",
    "the validation data set exactly once.\n",
    "- Next, we repeat this process for all of our different hyperparameter configurations to identify the optimal one.\n",
    "- Determining the search space of your hyperparameters can be difficult, and often\n",
    "doing a random search of hyperparameters outperforms a structured grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "112ad8ae"
   },
   "source": [
    "### To perform a hyperparameter search in Spark, take the following steps :\n",
    "1. Define the estimator you want to evaluate.\n",
    "2. Specify which hyperparameters you want to vary, as well as their respective values, using the ParamGridBuilder.\n",
    "3. Define an evaluator to specify which metric to use to compare the various\n",
    "models.\n",
    "4. Define an evaluator to specify which metric to use to compare the various\n",
    "models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06a678bd"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[stringIndexer,vecAssembler,rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e27c435e"
   },
   "source": [
    "- For our <b>ParamGridBuilder</b> , we’ll vary our <b>maxDepth</b> to be <b>2, 4, or 6</b> and <b>numTrees (the number of trees in our random forest)</b> to be <b>10 or 100</b>. This will give us a grid of <b>6 (3 x 2)</b> different hyperparameter configurations in total:<br><b>\n",
    "<center>(maxDepth=2, numTrees=10)</center>\n",
    "<center>(maxDepth=2, numTrees=100)</center>\n",
    "<center>(maxDepth=4, numTrees=10)</center>\n",
    "<center>(maxDepth=4, numTrees=100)</center>\n",
    "<center>(maxDepth=6, numTrees=10)</center>\n",
    "<center>(maxDepth=6, numTrees=100)</center></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a790ed5"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6d9f753"
   },
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2,4,6])\n",
    "             .addGrid(rf.numTrees, [10,100])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbb06f5b"
   },
   "source": [
    "- Now that we have set up our hyperparameter grid, we need to define how to evaluate\n",
    "each of the models to determine which one performed best. For this task we will use\n",
    "the <b>RegressionEvaluator</b> , and we’ll use <b>RMSE</b> as our metric of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "957d793c"
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol='price',\n",
    "                              predictionCol='prediction',\n",
    "                              metricName='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a745bf56"
   },
   "source": [
    "- We will perform our k-fold cross-validation using the <b>CrossValidator</b> , which accepts\n",
    "an <b>estimator , evaluator , and estimatorParamMaps</b> so that it knows which model to\n",
    "use, how to evaluate the model, and which hyperparameters to set for the model.\n",
    "- We can also set the number of folds we want to split our data into <b>(numFolds=3)</b>, as well as setting a seed so we have reproducible splits across the folds (seed=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "009a7534"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "549c1608"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline,\n",
    "                   evaluator=evaluator,\n",
    "                   estimatorParamMaps=paramGrid,\n",
    "                   numFolds=3,\n",
    "                   seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd4566e0",
    "outputId": "a675d74a-bf2c-4645-a0dc-fe3d322a0160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 s, sys: 520 ms, total: 3.44 s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f63e8912"
   },
   "source": [
    "- So, <b>how many models did we just train?</b> If you answered <b>18 (6 hyperparameter configurations x 3-fold cross-validation)</b>, <b><i>you’re close</i></b>. Once you’ve identified the optimal\n",
    "hyperparameter configuration, <b>how do you combine those three (or k) models together?</b> While some models might be easy enough to average together, some are not. Therefore, <b>Spark retrains your model on the entire training data set once it has identified the optimal hyperparameter configuration</b>, so in the end <b>we trained 19 models</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a247184",
    "outputId": "1697231a-4bc3-4d6a-bece-a5caa58e6160"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_dc713eadf42a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_dc713eadf42a', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  291.18226409247836),\n",
       " ({Param(parent='RandomForestRegressor_dc713eadf42a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_dc713eadf42a', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  286.7714750274078),\n",
       " ({Param(parent='RandomForestRegressor_dc713eadf42a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 4,\n",
       "   Param(parent='RandomForestRegressor_dc713eadf42a', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  287.6963245160818),\n",
       " ({Param(parent='RandomForestRegressor_dc713eadf42a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 4,\n",
       "   Param(parent='RandomForestRegressor_dc713eadf42a', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  279.99270572360797),\n",
       " ({Param(parent='RandomForestRegressor_dc713eadf42a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6,\n",
       "   Param(parent='RandomForestRegressor_dc713eadf42a', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  294.34810870889305),\n",
       " ({Param(parent='RandomForestRegressor_dc713eadf42a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6,\n",
       "   Param(parent='RandomForestRegressor_dc713eadf42a', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  275.3986270472998)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To inspect the results of the cross-validator, you can take a look at the avgMetrics :\n",
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a35f51e5"
   },
   "source": [
    "## Optimizing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "046f10ee"
   },
   "source": [
    "In the preceding code, even though each of the models in the crossvalidator is technically independent, spark.ml actually trains the collection of models <b>sequentially rather than in parallel</b>. In Spark 2.3, a parallelism parameter was introduced to solve this problem. This parameter determines the number of models to train in parallel, which themselves are fit in parallel. <b>From the Spark Tuning Guide:</b><br><br>\n",
    "<i><center>\"The value of parallelism should be chosen carefully to maximize parallelism without exceeding cluster resources, and larger values may not always lead to improved performance. Generally speaking, a value up to 10 should be sufficient for most clusters.\"<center><i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eaaa2c2",
    "outputId": "c7407322-77d2-4511-8893-688492085ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.74 s, sys: 565 ms, total: 3.31 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvModel = cv.setParallelism(4).fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39fe8512"
   },
   "source": [
    "- There’s another trick we can use to speed up model training:\n",
    "putting the cross-validator inside the pipeline.\n",
    "- Every time the cross-validator evaluates the pipeline, it runs\n",
    "through every step of the pipeline for each model, even if some of the steps don’t\n",
    "change, such as the StringIndexer .\n",
    "- By reevaluating every step in the pipeline, we are\n",
    "learning the same StringIndexer mapping over and over again, even though it’s not\n",
    "changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6c2148c"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fe4c7d7",
    "outputId": "8987edf6-284d-4eb9-c480-d16f95d4e0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 378 ms, total: 1.62 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8701bbc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ea9cb0"
   },
   "source": [
    "#### References:\n",
    "    1- https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "    2- https://spark.apache.org/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eaa29fe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture_4_Introduction to MLlib with supervised algorithms .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
